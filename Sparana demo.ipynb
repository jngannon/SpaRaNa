{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparana Demonstration\n",
    "\n",
    "This is a short notebook to demonstrate how to train a model using my sparse training library sparana. It demonstrates training, pruning, and retraining of a sparse model. I will briefly explain some planned features that I have not implemented yet. \n",
    "\n",
    "### Dependencies, and training data\n",
    "\n",
    "I have used numpy and cupy, these will need to be installed. I have used the tensorflow loader for the MNIST dataset because, it is there and easy to use. Not having cupy installed will throw an error.\n",
    "\n",
    "## Create the model\n",
    "\n",
    "I am trying to keep the code as clean as possible, cost functions and regularization is associated with training, so are defined with the optimizer function. Comp_type is set as either GPU to train on the graphics card using Cupy, or CPU using numpy, which I use for models that don't fit in GPU memory. \n",
    "\n",
    "mymodel.initialze_weights just initializes the weights using what is known as Xavier initialization, described here (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). I have also implemented different types of initializations, but Xavier initialization seems to work the best, and is the only one I tend to use. \n",
    "\n",
    "### Initialization ToDo:\n",
    "\n",
    "I have not implemented an initialization of sparse matrices yet. I have pruned trained models by removing more than 90% of parameters without losing performance (So have many others), and I intend to test this by training on weight matrices that are initialized as this sparse to begin with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Initalizing GPU weights\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "from sparana.model import model\n",
    "from sparana.layers import full_relu_layer\n",
    "from sparana.layers import full_linear_layer\n",
    "from sparana.optimizer import sgd_optimizer\n",
    "from sparana.lobotomizer import lobotomizer\n",
    "from sparana.optimizer import sgd_optimizer\n",
    "from sparana.data_loader import loader\n",
    "from sparana.lobotomizer import lobotomizer\n",
    "path = 'c:/users/jim/tensorflowtrials'\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)\n",
    "\n",
    "mymodel = model(input_size = 784, \n",
    "                layers = [full_relu_layer(size = 1500), \n",
    "                          full_relu_layer(size = 1000),\n",
    "                          full_relu_layer(size = 600),\n",
    "                          full_linear_layer(size = 10)],\n",
    "               comp_type = 'GPU')\n",
    "\n",
    "mymodel.initialize_weights('Xavier', bias_constant = 0.1)\n",
    "\n",
    "myloader = loader(mnist.train.images,\n",
    "                 mnist.train.labels,\n",
    "                 mnist.test.images,\n",
    "                 mnist.test.labels)\n",
    "\n",
    "opt = sgd_optimizer(mymodel, 0.0001, l2_constant = 0.0001)\n",
    "\n",
    "lobo = lobotomizer(mymodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader, optimizer and lobotomizer\n",
    "\n",
    "This loader just takes numpy arrays, and tracks how many minibatches, and epochs have been used, makes it easier for when I just re-run the same training loop. All data is stored in memory so this will not be fit for much larger datasets, but works fine for MNIST.\n",
    "\n",
    "The optimizer is pretty basic, L2 regularization set here rather than with the layers.\n",
    "\n",
    "Lobotomizer is the name I use for the module that analyses the parameters and prunes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myloader = loader(mnist.train.images,\n",
    "                 mnist.train.labels,\n",
    "                 mnist.test.images,\n",
    "                 mnist.test.labels)\n",
    "\n",
    "opt = sgd_optimizer(mymodel, learning_rate = 0.0001, l2_constant = 0.0001)\n",
    "\n",
    "lobo = lobotomizer(mymodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "The loader returns a tuple of inputs and labels, the optimizer just takes inputs and labels. Yes I know that using the loader instead of the tensorflow function for test data is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9833\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    images, labels = myloader.minibatch(150)\n",
    "    opt.train_step(images, labels)\n",
    "    \n",
    "print(mymodel.get_accuracy(myloader.test_data(), myloader.test_labels()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning\n",
    "\n",
    "Here I prune the 80% of weights with the smallest absolute value. 80% is not a very big advantage, this is for a demonstration. I have only built an SGD optimizer at this stage, but have tested others, Adam optimizer seems to encorage sparsity, and allow models to be more heavily pruned. 98.3% is not anything special, but for a basic fully connected model trained using stochastic gradient descent, it is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6696\n"
     ]
    }
   ],
   "source": [
    "lobo.get_absolute_values()\n",
    "lobo.prune_smallest(0.8)\n",
    "\n",
    "print(mymodel.get_accuracy(myloader.test_data(), myloader.test_labels()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the model and retraining\n",
    "\n",
    "Training on sparse matrices is slower than on full matrices, but gets faster as more parameters are removed, removing 80% will still be slower than regular training. The performance drop is too much to be of any use in this example, but does let me show that a model can be trained on sparse parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is now sparse\n",
      "0.9138\n"
     ]
    }
   ],
   "source": [
    "mymodel.convert_to_sparse()\n",
    "\n",
    "for i in range(5000):\n",
    "    images, labels = myloader.minibatch(150)\n",
    "    opt.train_step(images, labels)\n",
    "    \n",
    "print(mymodel.get_accuracy(myloader.test_data(), myloader.test_labels()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  286\n",
      "Minibatches:  105000\n"
     ]
    }
   ],
   "source": [
    "myloader.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More work to be done.\n",
    "\n",
    "Well, from an experiment point of view, this run is a failure, 91% accuracy on MNIST for this much work is no good to anyone, more tweaking needs to be done. \n",
    "\n",
    "From a software point of view, the library is working as intended, re-training a pruned, sparse model over a limited number of parameters improved performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

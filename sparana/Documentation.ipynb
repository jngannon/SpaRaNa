{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparana\n",
    "\n",
    "This is my library, if you want to use this with a GPU, you will need to get [CUPY](https://cupy.dev/) working. Look at [their documentation](https://docs.cupy.dev/en/stable/install.html) about setting it up. There are also some parts that use [Numba](https://numba.pydata.org/). This might be more detail that most of these have, I have included how it works as well as how to use it. This is an open source project, and if you want to modify/improve the code, you should have it explained a bit. For each section, the how to use is at the top, the rest is further down, you don't need to know everything about how the code works to get it to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model.py\n",
    "\n",
    "This file contains one class, the model class, which builds a model using layer objects. It has operations to initialize weights before training, produce outputs, convert between sparse and dense data structures, among other things. This class is passed into other Sparana classes which train and manipulate models.\n",
    "\n",
    "## Building and initializing.\n",
    "\n",
    "The code to build a model using the model object needs 2 inputs, the input size, and a python list of layer objects. The comp_type input here can be 'CPU' or 'GPU', it selects GPU automatically, so you don't need to define it here, but it is left in here for demonstration purposes. \n",
    "\n",
    "See the layers section for more information about layers, the important thing to note here is that the final layer size is the number of classes the model will have, this is for MNIST so there are 10 classes.\n",
    "\n",
    "```python\n",
    "mymodel = model(input_size = 784,\n",
    "                layers = [full_relu_layer(size = 1000), \n",
    "                          full_relu_layer(size = 800),\n",
    "                          full_relu_layer(size = 400),\n",
    "                          full_linear_layer(size = 10)],\n",
    "                comp_type = 'GPU')\n",
    "```\n",
    "This example can be found in the Demo-Model notebook, first code cell, line 21.\n",
    "\n",
    "## Functions for the user\n",
    "\n",
    "### Outputs\n",
    "\n",
    "Pass in some inputs in a numpy array, this will output a numpy array of the output of the final layer of the model.\n",
    "```python\n",
    "this_output = model.outputs(intput) \n",
    "```\n",
    "This example can be found in the Demo-Model notebook, 3rd code cell, line 3.\n",
    "\n",
    "### Partial outputs\n",
    "\n",
    "This gives the outputs of one of the layers within the model. This is used for some type of selected parameter training. I can get outputs from the second last layer of a 5 layer model with\n",
    "```python\n",
    "next_input = model.partial_outputs(input, 3)\n",
    "```\n",
    "next_input can then be used for the input for a different final layer/layers.\n",
    "\n",
    "### Get accuracy\n",
    "\n",
    "Given an input, and a label vector, this will give the accuracy. It matches the argmax of each output, to the given label.\n",
    "```python\n",
    "accuracy = mymodel.get_accuracy(input, labels)\n",
    "```\n",
    "\n",
    "This example can be found in the Demo-Model notebook, 3rd code cell, line 21.\n",
    "\n",
    "### Initialize weights\n",
    "\n",
    "This can initialize weights several different ways:\n",
    "```python\n",
    "model.initialize_weights('Xavier', bias_constant = 0.1)\n",
    "```\n",
    "is Xavier initialization, probably the most popular, just google it, biases are initialized as a constant.\n",
    "```python\n",
    "model.initialize_weights(0.1, bias_constant = 0.1) \n",
    "```\n",
    "Any float in the place of 0.1 will draw from a normal distribution centred at 0 with a standard deviation of 0.1 (or chosen float), biases are initialized as a constant.\n",
    "```python\n",
    "parameter_list = [(weight1, bias1), (weight2, bias2) etc.]\n",
    "\n",
    "model.initialize(parameter_list)\n",
    "```\n",
    "\n",
    "Will load weights and biases from a list of weight, bias pairs. This is one way of loading a model that I added to the libaray, but never actually used.\n",
    "\n",
    "This example can be found in the Demo-Model notbook, first cell, line 31\n",
    "\n",
    "### Initialize sparse weights\n",
    "\n",
    "This initializes sparse weights that are randomly distributed, it does not work well, and is slow to train, so I am not putting in too much effort explaining it. There might be some potential for structured initializations, convolutional neural nets are sort of like this, and I will update this if I add that sort of functionallity to this.\n",
    "\n",
    "*** Do a demo so that I can add other types of sparse inits***\n",
    "\n",
    "### Convert comp type\n",
    "\n",
    "Converts from GPU to CPU data structures, or the other way. This is a simple function that calls the convert layer comp type function in the layer class. Run it using:\n",
    "\n",
    "model.convert_comp_type()\n",
    "\n",
    "This example can be found in Demo-Model, 4th code cell.\n",
    "\n",
    "### Convert to sparse\n",
    "\n",
    "Converts a full/dense model into a sparse model. To this by using:\n",
    "\n",
    "model.convert_to_sparse()\n",
    "\n",
    "I have not built a module to convert models the other way. It would not be too difficult, but I find the best way of working with models is to do everything I need to on dense models, then convert them. I have not had a need to convert models the other way. \n",
    "\n",
    "Note. Remove activations below only works for dense models, I just have to remove the activations before converting, rather than reworking that function for sparse structures.\n",
    "\n",
    "*** Demo for this is in the pruned model notebook***\n",
    "\n",
    "### Remove activations\n",
    "\n",
    "After pruning there can be activations where all of the weights are set to 0, the only effect that this has on the model overall is the biases, which are constant. These can then be added to next layer, and the activations(martix columns) can be removed. This it done by calling:\n",
    "\n",
    "model.remove_actuvations()\n",
    "\n",
    "Not quite finished, need to add the bit where it prints the final size of the models.\n",
    "\n",
    "*** Puned model notebook\n",
    "\n",
    "## To Do\n",
    "\n",
    "These are things that I have not built, or finished that may be useful in the future.\n",
    "\n",
    "### Top 5 accuracy\n",
    "\n",
    "## Functions for other functions\n",
    "\n",
    "Everything in this class is for use by the user\n",
    "\n",
    "## Attributes\n",
    "\n",
    "These are class attributes used with the python self parameter, I use the standard of an underscore for attributes to distinguish from functions, eg. input_size is self._input_size. \n",
    "\n",
    "#### input_size\n",
    "The size of the input vector, used to initialize the first weight matrix.\n",
    "#### layers\n",
    "The list of layer objects\n",
    "#### dropout\n",
    "This sets the number of parameters that are set to 0 if dropout is used during training. This is actually drop connect, which zeros weights, where dropout zeros activations. This is slower, more computation/memory intensive. I should probably add dropout sometime soon. \n",
    "#### comp_type\n",
    "Computation type, lets other parts of the software know if the model is run on the CPU or the GPU. Stored as a string, 'CPU' or 'GPU. \n",
    "#### depth\n",
    "The number of layers, just an easy to read reference. Makes my code easier to understand than len(model._layers).\n",
    "#### layer_type\n",
    "Dense or sparse, inferred from the first layer, I have not worked yet with mixed sparse, dense models so this works fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers.py\n",
    "\n",
    "This file contains 6(5) classes, they layer objects that are put into a model object to make a working model. There are different types for each activation. This is because the gradients are calculated within this object. This is a design that I chose, one of the reasons for building this project was to understand backpropogation better. \n",
    "\n",
    "All classes have the same functions and attributes, mostly (I havn't put everything into the sparse layers yet).\n",
    "\n",
    "To build and train a model, the only way you will need to use these objects is to import them and put them in the model object during initilization like this:\n",
    "\n",
    "```python\n",
    "mymodel = model(input_size = 784, \n",
    "                layers = [full_relu_layer(size = 1000), \n",
    "                          full_relu_layer(size = 800),\n",
    "                          full_relu_layer(size = 400),\n",
    "                          full_linear_layer(size = 10)],\n",
    "                comp_type = 'GPU')\n",
    "```\n",
    "\n",
    "## Layer types\n",
    "\n",
    "#### full_relu_layer\n",
    "\n",
    "#### full_linear_layer\n",
    "\n",
    "#### full softmax_layer\n",
    "\n",
    "#### sparse_relu_layer\n",
    "\n",
    "#### sparse_linear_layer\n",
    "\n",
    "#### sparse_softmax_layer\n",
    "\n",
    "I have not built this layer yet, I will when I am working on a project that needs one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lobotomizer\n",
    "\n",
    "This contains the classes I use for pruning models, among other things. There are 3 main classes that are for the user, and some additional functions that are used by these classes. \n",
    "\n",
    "## Lobotomizer\n",
    "\n",
    "Removes weights from a neural network based on one of several selection criteria, performs a lobotomy, hence the dumb name. \n",
    "\n",
    "\n",
    "## Vulcanizer\n",
    "\n",
    "Splits submodels off models for memory efficient training, the name is another dumb joke, puting the submodel parameters back into the main model is a mind meld, like the vulcans from star trek. I will write a full notebook about what this does somewhere else, this just explains the functions\n",
    "\n",
    "\n",
    "## Parameter selector\n",
    "\n",
    "Selects parameters for partial model training, sort of a precursor to vulcanizer, this is for training on a subset of parameters, it is not more memory efficient. Again I will write up a notebook about how to use it and why\n",
    "\n",
    "## Other functions\n",
    "\n",
    "These are called by the user with the class objects. \n",
    "\n",
    "### Get MAV\n",
    "\n",
    "\n",
    "### Get MAAV\n",
    "\n",
    "\n",
    "### Get absolute values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model mixer\n",
    "\n",
    "This is for an experiment that worked quite well, it combines the weights of different models, using several different methods. Similar to what is done by the vulcanizer, it is for a seperate set of experiments, they are different enough that it was worth building a seperate thing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "This is where I have build some optimizers, the ones that are useful are SGD and ADAM, there are some others that do not work very well, they were just fun projects that were never going to work too well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saver\n",
    "\n",
    "This is for storing and saving models, pretty simple to use, but quite useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracker\n",
    "\n",
    "This is for tracking what goes on between layers. It has been the start of an experiment that is interesting, but I have not followed it up enough to know if it is useful. I track the cosine distance between "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader\n",
    "\n",
    "This seems like it would be simple, but I have added a couple of different things for different experiments. Sometimes I need to split the data up in different ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "I need a couple of VERY concise examples of how to use different things here\n",
    "\n",
    "## Simple build and train\n",
    "\n",
    "\n",
    "## Prune and convert\n",
    "\n",
    "\n",
    "## Forgetting and not forgetting\n",
    "\n",
    "This can demonstrate the parameter selector, and the data loader bits, and show some interesting results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
